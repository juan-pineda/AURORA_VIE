{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module: spectrom_tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import logging\n",
    "import astropy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import astropy.convolution\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from . import constants as ct\n",
    "from . import emitters as emit\n",
    "from . import convolutions as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __project_all_chunks(geom, run, spectrom, data_gas):\n",
    "    \"\"\"\n",
    "    Split gas particles into several chunks according to *nvector* and\n",
    "    compute the projected flux one by one. The resulting structure is 4D\n",
    "    \n",
    "    :param geom: geom object.\n",
    "    :type geom: aurora.configuration.GeometryObj\n",
    "    :param run: run object.\n",
    "    :type run: aurora.configuration.RunObj\n",
    "    :param spectrom: spectrom object.\n",
    "    :type spectrom: aurora.configuration.SpectromObj\n",
    "    :param data_gas: Gas array containing gas particles\n",
    "    identified in the input archive.\n",
    "    :type data_gas: pynbody.snapshot.FamilySubSnap\n",
    "    :return cube: 4D-array, contains the fluxes at each\n",
    "    pixel and velocity channel produced by the gas \n",
    "    particles with a given smoothing lengths separately. \n",
    "    :type cube: numpy.ndarray    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Code flow:\n",
    "    # =====================\n",
    "    # > Creates the 4D output array\n",
    "    # > Define the number of chunks\n",
    "    # > Project and add the fluxes iteratively\n",
    "\n",
    "    nchunk = int(math.ceil(len(data_gas) / float(run.nvector)))\n",
    "\n",
    "    if run.ncpu > 1:\n",
    "        return get_cube_in_parallel(geom, run, spectrom, data_gas, nchunk)\n",
    "    else:\n",
    "        return get_cube_in_sequential(geom, run, spectrom, data_gas, nchunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cube_in_parallel(geom, run, spectrom, data_gas, nchunk):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cube_side, n_ch = spectrom.cube_dims()\n",
    "    cube_size = np.zeros((n_ch, cube_side, cube_side, run.nfft)).nbytes\n",
    "    memory_needed_ncores = int((cube_size/1e6) * min(run.ncpu, nchunk))\n",
    "    memory_available = int(os.popen(\"free -m\").readlines()[1].split()[-1])\n",
    "\n",
    "    num_cores = int(run.ncpu)\n",
    "    if memory_available > memory_needed_ncores:\n",
    "        cube_list = Parallel(n_jobs=num_cores)(delayed(__project_spectrom_flux)\n",
    "                                               (geom, run, spectrom, data_gas, i) for i in range(nchunk))\n",
    "        return sum(cube_list)\n",
    "    else:\n",
    "        logging.warning(f\"Not enough RAM left in your device for this operation in parallel.\")\n",
    "        logging.info(f\"Needed {memory_needed_ncores}Mb, you have {memory_available}Mb Free.\")\n",
    "        logging.info(f\"Using a single cpu mode...\")\n",
    "        return get_cube_in_sequential(geom, run, spectrom, data_gas, nchunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cube_in_sequential(geom, run, spectrom, data_gas, nchunk):\n",
    "    \"\"\"\n",
    "    Determine the availability of RAM memory to carry out the flow\n",
    "    projection process, and sets the upper and lower limits on gas \n",
    "    particles according to *nchunks*\n",
    "\n",
    "    :param geom: geom object.\n",
    "    :type geom: aurora.configuration.GeometryObj\n",
    "    :param run: run object.\n",
    "    :type run: aurora.configuration.RunObj\n",
    "    :param spectrom: spectrom object.\n",
    "    :type spectrom: aurora.configuration.SpectromObj\n",
    "    :param data_gas: Gas array containing gas particles\n",
    "    identified in the input archive.\n",
    "    :type data_gas: pynbody.snapshot.FamilySubSnap\n",
    "    :param int nchunk: number of chunks to divide the gas particles.\n",
    "    :return cube: 4D-array, contains the fluxes at each\n",
    "    pixel and velocity channel produced by the gas \n",
    "    particles with a given smoothing lengths separately. \n",
    "    :type cube: numpy.ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    cube_side, n_ch = spectrom.cube_dims()\n",
    "    cube_size = np.zeros((n_ch, cube_side, cube_side, run.nfft)).nbytes\n",
    "    memory_needed_1core = int(cube_size/1e6)\n",
    "    memory_available = int(os.popen(\"free -m\").readlines()[1].split()[-1])\n",
    "\n",
    "    if memory_available > memory_needed_1core:\n",
    "        if abs(memory_available-memory_needed_1core) < 1000:\n",
    "            logging.warning(f\"Your computer may be slow during this operation, be patient.\")\n",
    "        cube = np.zeros((n_ch, cube_side, cube_side, run.nfft))\n",
    "        for i in tqdm(range(nchunk)):\n",
    "            start = i * run.nvector\n",
    "            stop = start + min(run.nvector, len(data_gas) - start)\n",
    "            __project_spectrom_flux(\n",
    "                geom, run, spectrom, data_gas, start, stop, cube)           \n",
    "        return cube\n",
    "    else:\n",
    "        raise MemoryError(f\"Not enough RAM in your device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __project_spectrom_flux(geom, run, spectrom, data_gas, *args):\n",
    "    \"\"\"\n",
    "    Compute the H-alpha emission of a bunch of particles and project it\n",
    "    to a 4D grid, keeping contributions from different scales separated\n",
    "\n",
    "    :param geom: geom object.\n",
    "    :type geom: aurora.configuration.GeometryObj\n",
    "    :param run: run object.\n",
    "    :type run: aurora.configuration.RunObj\n",
    "    :param spectrom: spectrom object.\n",
    "    :type spectrom: aurora.configuration.SpectromObj\n",
    "    :param data_gas: Gas array containing gas particles\n",
    "    identified in the input archive.\n",
    "    :type data_gas: pynbody.snapshot.FamilySubSnap\n",
    "    :param *args: number of chunks to divide the gas\n",
    "    particles, or list of gas particles to be projected.\n",
    "    :type *args: list, numpy.ndarray\n",
    "    :return cube: 4D-array, contains the fluxes at each\n",
    "    pixel and velocity channel produced by the gas \n",
    "    particles with a given smoothing lengths separately. \n",
    "    :type cube: numpy.ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    cube_side, n_ch = spectrom.cube_dims()\n",
    "    if len(args) == 1:\n",
    "        i = args[0]\n",
    "        start = i * run.nvector\n",
    "        stop = start + min(run.nvector, len(data_gas) - start)\n",
    "        cube = np.zeros((n_ch, cube_side, cube_side, run.nfft))\n",
    "    else:\n",
    "        start, stop, cube = args\n",
    "        \n",
    "    # This object allows to calculate the Halpha flux, and line broadening\n",
    "    em = emit.Emitters(data_gas[start:stop], spectrom.redshift_ref)\n",
    "    em.get_state()\n",
    "    # em.density_cut(spectrom.density_cut) # new feature in test !!!\n",
    "    em.get_luminosity(spectrom.lum_dens_rel, spectrom.density_cut) # new feature in test !!!\n",
    "    em.get_vel_dispersion()\n",
    "\n",
    "    x, y, index = spectrom.position_in_pixels(em.x,em.y)\n",
    "\n",
    "    # scale to which each particle belongs according to its smoothing lenght\n",
    "    scale = np.digitize(em.smooth.to(\"kpc\"), 1.1 * run.fft_hsml_limits.to(\"kpc\"))\n",
    "\n",
    "    line_center, line_sigma, line_flux = em.get_vect_lines(n_ch)\n",
    "    channel_center, channel_width = em.get_vect_channels(spectrom.vel_channels, spectrom.velocity_sampl, n_ch)\n",
    "    \n",
    "    # Spectral convolution\n",
    "    if(spectrom.spectral_res > 0):\n",
    "        psf_fwhm = ct.c/spectrom.spectral_res\n",
    "        psf_sigma = psf_fwhm / ct.fwhm_sigma\n",
    "        line_sigma = np.sqrt(line_sigma**2+psf_sigma**2)\n",
    "\n",
    "    # Integrated flux inside each velocity channel given its position and width\n",
    "    flux_in_channels = em.int_gaussian_with_units(channel_center, channel_width, line_center,\n",
    "        line_sigma) * line_flux\n",
    "\n",
    "    # Divide by the effective channel width\n",
    "    flux_in_channels = flux_in_channels.to(\"erg s^-1\").value / spectrom.velocity_sampl.to(\"km s^-1\").value / geom.dl.to(\"cm\").value**2\n",
    "    \n",
    "    # Compute the fluxes scale by scale\n",
    "    for i in np.unique(scale):\n",
    "        ok_level = np.where(scale == i)[0]\n",
    "        nok_level = ok_level.size\n",
    "        \n",
    "        # Unique indices (pixels) to which particles in this group contribute\n",
    "        unique_val, unique_ind = np.unique(index[ok_level], return_index=True)\n",
    "\n",
    "        eff_flux = flux_in_channels[ok_level]\n",
    "\n",
    "        # Sum all the lines for a given index\n",
    "        for j in range(unique_val.size):\n",
    "            to_sum = np.where(index[ok_level] == unique_val[j])[0]\n",
    "            eff_flux[unique_ind[j], :] = np.sum(eff_flux[to_sum, :], axis=0)\n",
    "        # Remove duplicated emission lines\n",
    "        eff_flux = eff_flux[unique_ind, :]\n",
    "        # Insert the line fluxes in the right positions at the right scale\n",
    "\n",
    "        cube[:, y[ok_level[unique_ind]],\n",
    "             x[ok_level[unique_ind]], i] += np.transpose(eff_flux)\n",
    "    return cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __cube_convolution(geom, run, spectrom, cube):\n",
    "    \"\"\"\n",
    "    Perform the spatial smoothing of fluxes projected to a 4D-grid.\n",
    "    Consider two kernels: the multi-scale kernels of the simulation and\n",
    "    the spatial PSF if it was defined\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    cube_side, n_ch = spectrom.cube_dims()\n",
    "    \n",
    "    for i in range(run.nfft):\n",
    "        logging.info(f\"Preparing for spatial smoothing, kernel = {round(run.fft_hsml_limits[i].value*1000, 1)} pc\")\n",
    "        sys.stdout.flush()\n",
    "        # Kernel smoothing\n",
    "        scale_fwhm = (run.fft_hsml_limits[i] / spectrom.pixsize).decompose().value\n",
    "        scale_sigma = spectrom.kernel_scale * scale_fwhm / ct.fwhm_sigma\n",
    "        logging.info(f\"Size of the kernel in pixels = {round(scale_sigma, 1)}\")\n",
    "        # Enlarge the kernel adding the effect of the PSF\n",
    "        if(spectrom.spatial_res_kpc > 0):\n",
    "            logging.info(f\" (Including the effect of the PSF as well)\")\n",
    "            # psf_fwhm = spectrom.spatial_res.value / spectrom.spatial_sampl.value\n",
    "            psf_fwhm = spectrom.spatial_res_kpc.to(\n",
    "                \"pc\").value / spectrom.pixsize.to(\"pc\").value\n",
    "            psf_sigma = psf_fwhm / ct.fwhm_sigma\n",
    "            logging.info(f\"Size of the PSF in pixels = {round(psf_sigma, 1)}\")\n",
    "            scale_sigma = np.sqrt(scale_sigma**2+psf_sigma**2)\n",
    "            logging.info(f\"Combination kernel + PSF in pixels = {round(scale_sigma, 1)}\")\n",
    "        if (scale_sigma <= 0.5):\n",
    "            logging.info(f\"-- Small kernel -> skip convolution\")\n",
    "            sys.stdout.flush()\n",
    "            continue\n",
    "            \n",
    "    for j in range(n_ch):\n",
    "            if (np.nanmax(cube[j, :, :, i]) == 0):\n",
    "                logging.info(f\"No flux at this scale/velocity channel -> skip convolution\")\n",
    "                continue\n",
    "            side = cv.next_odd(20*scale_sigma)  \n",
    "            psf = astropy.convolution.Gaussian2DKernel(scale_sigma, x_size=side, y_size=side)\n",
    "\n",
    "            channel = astropy.convolution.convolve(cube[j,:,:,i],psf)\n",
    "            cube[j, :, :, i] = 0.\n",
    "            cube[j, :, :, i] += channel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
